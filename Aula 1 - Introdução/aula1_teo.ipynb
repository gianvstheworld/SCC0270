{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 1 TEORIA - INTRODUÇÃO\n",
    "\n",
    "17/08/2022 - Aula Presencial\n",
    "\n",
    "---\n",
    "\n",
    "## Introdução ao Aprendizado Profundo\n",
    "\n",
    "- Apresentação da disciplina\n",
    "- Histórico de redes neurais e definições\n",
    "- Introdução às bibliotecas utilizadas\n",
    "- Rede neural linear\n",
    "- Implementação básica orientada a objetos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos Iniciais\n",
    "\n",
    "- **Atibutos:** Características do objeto de análise\n",
    "    - Exemplos: medir a similaridade de duas imagens, podemos dizer que os pixels das imagens são atributos (não é muito bom)\n",
    "    - Uma opção melhor é codificar as cores em 64 valores e representar as imagens por dois valores relativos as duas cores mais frequentes\n",
    "\n",
    "* **Classificador:** é o modelo (e não o algoritmo) criado a partir de um conjunto de dados anotado.\n",
    "\n",
    "*obs: O erro do classificador é o erro de treinamento*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline de reconhecimento de imagens**\n",
    "\n",
    "![alt text](pipeline_NN.jpg \"Title\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudando o pipeline do aprendizado\n",
    "\n",
    "**Primavera das redes neurais**\n",
    "* 1958: Frank Rosenblatt propõe o Perceptron como um modelo conexionista bioinspirado\n",
    "\n",
    "- Limitações em treinar o Perceptron original foram encontradas\n",
    "\n",
    "**Inverno das redes neurais**\n",
    "\n",
    "- de 1970 até 2010 grande parte da pesquisa em \"Inteligência Artificial\" se referia a criar sistemas especialistas\n",
    "    - havia maior atenção para métodos com embasamento teórico e garantias matemáticas (como SVM)\n",
    "\n",
    "**Nova Primavera das redes neurais**\n",
    "* Pesquisadores que trabalharam no inverno (1980’s): Rumelhart e McClelland (1986)\n",
    "\n",
    "- *Porque a nova primavera?:*  \n",
    "    - Disponibilidade de dados anotados\n",
    "    - Poder de processamento gráfico paralelo (GPU's)\n",
    "\n",
    "* Avanços -> Exemplo: Redes neurais convolucionais profundas\n",
    "\n",
    "![alt text](encoders.jpg \"Title\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning x Deep Learning\n",
    "\n",
    "* Terminologia: \n",
    "    * Instância: (objeto/exemplo) de entrada x\n",
    "    * Alvo: (rótulo ou outro) de saída\n",
    "    * Dataset: Conjunto de dados\n",
    "    * Label: valor “verdadeiro” atribuído a cada exemplo\n",
    "\n",
    "- Exemplo: **regressão: valor de um imóvel com base em suas características** \n",
    "    - **Dados disponíveis:** pares (características de um imóvel, valor)\n",
    "    - **Entrada:** metragem quadrada, localização, quantidade de quartos, organizados na forma x \n",
    "    - **Saída:** valor y (e.g. R$ 750 mil) de um determinado imóvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "* Machine Learning > Deep Learning\n",
    "\n",
    "- *Métodos rasos (“shallow”)* comumente inferem uma única f(x). e.g. uma função linear f (x) = w · x + b,\n",
    "    - Aprendizado: ajustar w e b\n",
    "    - Exemplos: Perceptron, Support Vector Machines (SVM), Logistic Regression Classifier, Linear Discriminant Analysis (LDA).\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "- Múltiplas representações são aprendidas de forma hierárquica por funções compostas.\n",
    "    - Exemplo: fn (· · · f3(f2(f1(x1, Θ1), Θ2), Θ3)· · · , Θn),\n",
    "    - Sendo fL a saída do aninhamento de n funções, onde Θi são os parametros para cada função i\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural: Raso x Profundo\n",
    "\n",
    "### Montando um classificador raso\n",
    "\n",
    "* Seja Θ uma matriz W de pesos e um vetor b de termos *bias*\n",
    "    * f(Θ, x) = W * x + b\n",
    "    * x é a imagem e f os scores para possíveis classes de x\n",
    "\n",
    "- obs: slide 35 com exemplo do classificador: https://edisciplinas.usp.br/pluginfile.php/7249183/mod_resource/content/1/scc0270_01_introducao.pdf\n",
    "\n",
    "### Otimizando um classificador raso\n",
    "\n",
    "- Estimar o quanto podemos melhorar com uma **função perda/custo**\n",
    "    - Estatisticamente, minimizar a perda esperada (expected loss)\n",
    "\n",
    "* Exemplos de funções custo para perdas empíricas computadas com N exemplos\n",
    "\n",
    "![alt text](custos.jpg \"Title\")\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "- Otimizador Gradient Descent\n",
    "    - Usa-se a inclinação da função de perda com relação aos parâmetros do modelo\n",
    "    - Na prática computamos o gradiente numérico e buscamos pelo vale (mínimo) por meio da descida do gradiente (gradient descent)\n",
    "    - Buscar o **mínimo global** (o que muitas vezes é bem complexo)\n",
    "\n",
    "![alt text](gradient_descent.jpg \"Title\")\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* É computacionalmente caro calcular o gradiente para N grande, para isso, usamos o SGD\n",
    "    * aproximar a perda empírica usando um lote aleatório **minibatch** de instâncias: 10 até 2000+\n",
    "    * mais rápida e mais grosseiro (necessita + iterações)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
