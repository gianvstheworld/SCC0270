{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 5 TEORIA - Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "Aula 5 - Aula Presencial\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "- Treinando redes profundas em cenários reais\n",
    "    - Suposições para convergência e aprendizado\n",
    "    - Estratégias para melhorar generalização\n",
    "    - Normalização de dados\n",
    "- Transferência de apredendizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algumas suposições feitas\n",
    "\n",
    "- Dados de treinamento \n",
    "    - Limpos\n",
    "    - Representativos e bem definidos com relação à tarefa: classes, valores da regressão, etc.\n",
    "    - Baixa taxa de erros de rótulo\n",
    "    - Quantidade de dados é suficiente \n",
    "- E se não for possível?\n",
    "    - Riscos: _overfitting_, baixa generalização, maior dificuldade no treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexidade de modelos\n",
    "\n",
    "- Lembrando: Aprendizado de Máquina pode ser formulado como sendo aprender os parâmetros de $f : X -> Y$\n",
    "- Um algoritmo ajusta $f$ a partir de um espaço de funções admissíveis $F$:\n",
    "    - \"muitas\" funções: mais graus de liberdade, menor garantia de convergência, possível overfitting;\n",
    "    - \"poucas\" funções: menos grau de liberdade, maior garantia de convergência, possível underfitting\n",
    "\n",
    "### Erros quando definindo o espaço de funções admissíveis\n",
    "\n",
    "<img src=\"Imagens/erros.png\" width=\"500\"> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilema víes e variância e a hipótese do bilhete de loteria\n",
    "\n",
    "<img src=\"Imagens/vies_variancia.png\" width=\"500\"> \n",
    "\n",
    "- Observa-se pelo gráfico que quanto maior a complexidade da função que rege a rede, maior os problemas existentes!\n",
    "\n",
    "<img src=\"Imagens/overparametrization.png\" width=\"500\"> \n",
    "\n",
    "-  Esse outro gráfico apresenta uma nova teoria estudada que trabalha com uma hipótese:\n",
    "    - Supondo que se inicialize aleatoriamente uma rede densa com $\\theta_{0}$\n",
    "    - Treinando a rede até atingir a convergência com parâmetros $\\theta$\n",
    "    - Podar $\\theta$ e criar uma máscara $m$\n",
    "    - A configuração de inicialização \"winning ticket\" é $\\theta$ x $m$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estratégias para melhorar generalização\n",
    "\n",
    "* Regularização\n",
    "    * Relembrando a regularização L2 (ou de Tikhonov)\n",
    "        * <img src=\"Imagens/regular.png\" width=\"500\"> \n",
    "        * Objetivo: limitar a capacidade do modelo de se especializar demais nos dados \n",
    "        * Formas:\n",
    "            * Global: na função de perda ponderada por $\\lambda$\n",
    "            * Definindo $\\lambda$ por camada (ou grupos de camadas)\n",
    "        * Interpretação: vê cada entrada como sendo de maior variância\n",
    "\n",
    "- Dropout\n",
    "    - Objetivo: limitar a capacidade de certos parâmetros do modelo a memorizarem os dadoss\n",
    "    - Implementado na forma de \"camada\"\n",
    "    - Em cada iteração, desliga ativações de neurônios aleatoriamente com probabilidade $p$\n",
    "    - Interpretação: treinamento com técnica \"Bagging\"\n",
    "    - <img src=\"Imagens/dropout.png\" width=\"500\"> \n",
    "\n",
    "* Parada precoce \n",
    "    * Objetivo: evitar que o modelo memorize os dados de treinamento ao treinar por muitas épocas \n",
    "    * Acompanhar um conjunto de validação e interromper de acordo com a relação do custo no treinamento e validação\n",
    "    * <img src=\"Imagens/precoce.png\" width=\"500\"> \n",
    "\n",
    "- Coletar mais dados\n",
    "    - Objetivo: impedir que o treinamento considere apenas um conjunto limitado de exemplos\n",
    "    - Baseado na lei dos grandes números, quanto maior a amostra, teremos um melhor estimador\n",
    "\n",
    "* Aumentação de dados/Data augmentation\n",
    "    * Objetivo: gerar exemplos artificiais na esperança de que melhore as propriedades de convergência\n",
    "    * Implementado por meio da manipulação de exemplos existentes, ou sua combinação\n",
    "    * Exemplos: \n",
    "        * Dados estruturados: SMOTE\n",
    "        * Não estruturados: rotação, corte, injeção de ruído, e outros que não descaracterizem os dados\n",
    "        * Dropout na camada de entrada: eliminando features aleatoriamente a cada iteração.\n",
    "    * __Dica para melhoria de performance final__\n",
    "        * Para cada exemplo de teste:\n",
    "            1. Gerar $m$ exemplos com aumentação de dados\n",
    "            2. Predizer o resultado para os $m$ exemplos\n",
    "            3. Combinar as predições: por média, maioria ou outro método\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização de dados\n",
    "\n",
    "* Exemplo de técnicas:\n",
    "    * __Normalização (ou padronização) z-score__: valores com média zero e desvio padrão 1\n",
    "    * __Normalização min-max__: valores no intervalo 0-1\n",
    "\n",
    "- Objetivo: facilitar otimização ao normalizar/padronizar a magnitude dos valores utilizados no treinamento\n",
    "    - Suaviza as ativações dos neurônios, reduzindo a variância do gradiente;\n",
    "    - Ataca o problema de \"desaparecimento\" do gradiente (vanishing gradient) em particular para redes profundas.\n",
    "\n",
    "* Podemos usar como pré-processamento considerando todos os dados de treinamento\n",
    "* Porém, durante o treinamento pode também ser aplicado ao batch ou às camadas\n",
    "* Tipos de normalização baseada em camadas\n",
    "    * Batch\n",
    "    * Camada \n",
    "    * Instância\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
