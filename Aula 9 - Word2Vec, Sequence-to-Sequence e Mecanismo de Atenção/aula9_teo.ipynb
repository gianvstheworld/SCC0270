{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 TEORIA - Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "Aula 9 - Aula síncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "- Word2Vec\n",
    "- Sequence-to-Sequence\n",
    "- Mecanismo de Atenção\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: representações para texto\n",
    "\n",
    "Representação para palavras\n",
    "- Sentence Embeddings\n",
    "- Word Embeddings\n",
    "- Char Embeddings\n",
    "\n",
    "Agora, tratando-se da função de custo para aprender essa representação, tem-se:\n",
    "\n",
    "<img src=\"Imagens/custo.png\" width=\"500\"> \n",
    "\n",
    "Otimiza em função de palavras que devem estar próximas se estiverem no mesmo contexto\n",
    "\n",
    "### Skip-grams (SG)\n",
    "Predição de palavras em uma certa \"janela\" de proximidade $m$ de uma palavra $t$\n",
    "\n",
    "Formulação \"softmax\"\n",
    "\n",
    "<img src=\"Imagens/softmax.png\" width=\"500\">\n",
    " \n",
    "Dada uma representação one-hot de uma palavra, calculamos sua representação vetorial e a multiplicamos pela matriz de pesos $W$.\n",
    "\n",
    "<img src=\"Imagens/w.png\" width=\"500\">\n",
    "\n",
    "Assim, $v_{c}$ é filtrada por representações $u_0$ das palavras de saída (no contexto que queremos prever) em diferentes posições $t - i$.\n",
    "\n",
    "<img src=\"Imagens/filtro.png\" width=\"200\">  \n",
    "\n",
    "Para todas as palavras do vocabulário isso é codificado em uma matriz:\n",
    "\n",
    "<img src=\"Imagens/codifica.png\" width=\"500\">\n",
    "\n",
    "* W aprende representações (nas colunas) para cada palavra quando são \"centrais\"\n",
    "* $U_{0}$ aprende representações (nas linhas) para cada palavra quando são \"contexto\"\n",
    "\n",
    "<img src=\"Imagens/representa.png\" width=\"500\">\n",
    "\n",
    "### Word2Vec: GloVe (Global Vector for Word Representation)\n",
    "\n",
    "<img src=\"Imagens/glove.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "### RNNs e Sequence-to-Sequence\n",
    "\n",
    "<img src=\"Imagens/seq2seq.png\" width=\"500\">\n",
    "\n",
    "NÃO É UM AUTOENCODER!\n",
    "\n",
    "### Mecanismo de atenção: intuição e motivação \n",
    "\n",
    "- Encontrar qual parte de uma sequência é mais importante para predizer uma certa saída\n",
    "- Em unidades recorrentes, cada entrada perturba a memória prejudicando conhecimento de dados anteriores\n",
    "\n",
    "Implemenação básica \n",
    "* Computar o alinhamento/similaridade entre o sumário atual do decoder, $s_{i}$, com sumários anteriores do encoder, $h_{j}$\n",
    "* Usa a softmax para obter pesos na forma de probabilidades\n",
    "\n",
    "<img src=\"Imagens/atencao.png\" width=\"500\">\n",
    "\n",
    "* Atenção produz um vetor de \"contexto\" que é combinado com o vetor de estado atual do decoder para produzir o vetor de estado atualizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Networks\n",
    "\n",
    "### RNNs vs Transformers Networks\n",
    "\n",
    "RNNs\n",
    "* Podem não funcionar com dependências longas\n",
    "* Recorrência dificulta computação paralela (pontos da sequência não podem ser processados em paralelo)\n",
    "* Podem sofrer com explosão ou desaparecimento do gradiente\n",
    "\n",
    "Transformers Networks\n",
    "- Não possui recorrência, apenas atenção\n",
    "- Facilita capturar dependências longas\n",
    "- Facilita processamento paralelo: atenção é invariante à permutação \n",
    "\n",
    "### Arquitetura \n",
    "\n",
    "<img src=\"Imagens/transformer.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
