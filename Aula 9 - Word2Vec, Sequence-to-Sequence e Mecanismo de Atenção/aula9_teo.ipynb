{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 TEORIA - Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "Aula 9 - Aula síncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "- Word2Vec\n",
    "- Sequence-to-Sequence\n",
    "- Mecanismo de Atenção\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: representações para texto\n",
    "\n",
    "Representação para palavras\n",
    "- Sentence Embeddings\n",
    "- Word Embeddings\n",
    "- Char Embeddings\n",
    "\n",
    "Agora, tratando-se da função de custo para aprender essa representação, tem-se:\n",
    "\n",
    "<img src=\"Imagens/custo.png\" width=\"500\"> \n",
    "\n",
    "Otimiza em função de palavras que devem estar próximas se estiverem no mesmo contexto\n",
    "\n",
    "### Skip-grams (SG)\n",
    "Predição de palavras em uma certa \"janela\" de proximidade $m$ de uma palavra $t$\n",
    "\n",
    "Formulação \"softmax\"\n",
    "\n",
    "<img src=\"Imagens/softmax.png\" width=\"500\">\n",
    " \n",
    "Dada uma representação one-hot de uma palavra, calculamos sua representação vetorial e a multiplicamos pela matriz de pesos $W$.\n",
    "\n",
    "<img src=\"Imagens/w.png\" width=\"500\">\n",
    "\n",
    "Assim, $v_{c}$ é filtrada por representações $u_0$ das palavras de saída (no contexto que queremos prever) em diferentes posições $t - i$.\n",
    "\n",
    "<img src=\"Imagens/filtro.png\" width=\"200\">  \n",
    "\n",
    "Para todas as palavras do vocabulário isso é codificado em uma matriz:\n",
    "\n",
    "<img src=\"Imagens/codifica.png\" width=\"500\">\n",
    "\n",
    "* W aprende representações (nas colunas) para cada palavra quando são \"centrais\"\n",
    "* $U_{0}$ aprende representações (nas linhas) para cada palavra quando são \"contexto\"\n",
    "\n",
    "<img src=\"Imagens/representa.png\" width=\"500\">\n",
    "\n",
    "### Word2Vec: GloVe (Global Vector for Word Representation)\n",
    "\n",
    "<img src=\"Imagens/glove.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "### RNNs e Sequence-to-Sequence\n",
    "\n",
    "<img src=\"Imagens/seq2seq.png\" width=\"500\">\n",
    "\n",
    "NÃO É UM AUTOENCODER!\n",
    "\n",
    "### Mecanismo de atenção: intuição e motivação \n",
    "\n",
    "- Encontrar qual parte de uma sequência é mais importante para predizer uma certa saída\n",
    "- Em unidades recorrentes, cada entrada perturba a memória prejudicando conhecimento de dados anteriores\n",
    "\n",
    "Implemenação básica \n",
    "* Computar o alinhamento/similaridade entre o sumário atual do decoder, $s_{i}$, com sumários anteriores do encoder, $h_{j}$\n",
    "* Usa a softmax para obter pesos na forma de probabilidades\n",
    "\n",
    "<img src=\"Imagens/atencao.png\" width=\"500\">\n",
    "\n",
    "* Atenção produz um vetor de \"contexto\" que é combinado com o vetor de estado atual do decoder para produzir o vetor de estado atualizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Networks\n",
    "\n",
    "### RNNs vs Transformers Networks\n",
    "\n",
    "RNNs\n",
    "* Podem não funcionar com dependências longas\n",
    "* Recorrência dificulta computação paralela (pontos da sequência não podem ser processados em paralelo)\n",
    "* Podem sofrer com explosão ou desaparecimento do gradiente\n",
    "\n",
    "Transformers Networks\n",
    "- Não possui recorrência, apenas atenção\n",
    "- Facilita capturar dependências longas\n",
    "- Facilita processamento paralelo: atenção é invariante à permutação \n",
    "\n",
    "### Arquitetura \n",
    "\n",
    "<img src=\"Imagens/transformer.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Método para pré-treinar encoders do tipo Transformer \n",
    "\n",
    "- BERT Base e BERT Large, modelos com bloco Transformer:\n",
    "    - Base:\n",
    "        - 12 camadas\n",
    "        - 12 cabeças de atenção\n",
    "        - 768 dimensões\n",
    "        - 110 milhões de parâmetros\n",
    "    - Large:\n",
    "        - 24 camadas\n",
    "        - 16 cabeças de atenção\n",
    "        - 1024 dimensões (Mais camadas e mais dimensões)\n",
    "        - 340 milhões de parâmetros\n",
    "\n",
    "Entrada formada por tokens (palavras ou parte de palavras) e segmentos (separação de sentenças). Além disso, existem palavras especiais para indicar o início e fim de uma sentença.\n",
    "\n",
    "<img src=\"Imagens/bert0.png\" width=\"500\">\n",
    "\n",
    "<img src=\"Imagens/bert1.png\" width=\"500\">\n",
    "\n",
    "* Em GloVe e embeddings similares\n",
    "    * Vetor fixo por palavra independente do contexto\n",
    "    * Exemplo: \"Eu gosto de cachorro\" e \"Eu gosto de gato\" tem a mesma representação para \"gosto\"\n",
    "* ELMo: olha para a sentença antes de atribuir um vetor para ela\n",
    "    * Usa LSTM biderecional para aprender representações de sentenças\n",
    "    * Aprende (sem labels) a predizer a próxima palavra (e a anterior)\n",
    "\n",
    "Segunda tarefa de pré-treinamento\n",
    "\n",
    "<img src=\"Imagens/label.png\" width=\"500\">\n",
    "\n",
    "ULM-FiT: usa métodos efeitovos para pré-treinar modelos de linguagem\n",
    "\n",
    "Datasets: book corpus (livros) e wikipedia\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
