{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 10 TEORIA - Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "Aula 10 - Aula síncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Word2Vec, Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "- Word2Vec\n",
    "- Sequence-to-Sequence\n",
    "- Mecanismo de Atenção\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: representações para texto\n",
    "\n",
    "Representação para palavras\n",
    "- Sentence Embeddings\n",
    "- Word Embeddings\n",
    "- Char Embeddings\n",
    "\n",
    "Agora, tratando-se da função de custo para aprender essa representação, tem-se:\n",
    "\n",
    "<img src=\"Imagens/lossW2V.png\" width=\"500\"> \n",
    "\n",
    "Otimize em função de palavras que devem estar próximas se estiverem no mesmo contexto\n",
    "\n",
    "### Skip-grams (SG)\n",
    "Predição de palavras em uma certa \"janela\" de proximidade $m$ de uma palavra $t$\n",
    "\n",
    "Formulação \"softmax\"\n",
    "\n",
    "<img src=\"Imagens/sg.png\" width=\"500\">\n",
    " \n",
    "Dada uma representação one-hot de uma palavra, calculamos sua representação vetorial e a multiplicamos pela matriz de pesos $W$.\n",
    "\n",
    "<img src=\"Imagens/sg.png\" width=\"500\">\n",
    "\n",
    "Assim, $v_{c}$ é filtrada por representações $u_0$ das palavras de saída (no contexto que queremos prever) em diferentes posições $t - i$.\n",
    "\n",
    "CONTINUAR ESCREVENDO AQUI   \n",
    "\n",
    "### Word2Vec: GloVe (Global Vector for Word Representation)\n",
    "\n",
    "<img src=\"Imagens/glove.png\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence e Mecanismo de Atenção\n",
    "\n",
    "### RNNs e Sequence-to-Sequence\n",
    "\n",
    "<img src=\"Imagens/rnn.png\" width=\"500\">\n",
    "\n",
    "NÃO É UM AUTOENCODER!\n",
    "\n",
    "### Mecanismo de atenção: intuição e motivação \n",
    "\n",
    "- Encontrar qual parte de uma sequência é mais importante para predizer uma certa saída\n",
    "- Em unidades recorrentes, cada entrada perturba a memória prejudicando conhecimento de dados anteriores\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
