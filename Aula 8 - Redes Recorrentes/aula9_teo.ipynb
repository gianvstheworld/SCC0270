{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 TEORIA - Redes Recorrentes\n",
    "\n",
    "Aula 9 - Aula Assíncrona\n",
    "https://www.youtube.com/watch?v=OzC_vwAqRTU\n",
    "\n",
    "---\n",
    "\n",
    "## Redes Recorrentes\n",
    "\n",
    "- Dados sequenciais: recorrência\n",
    "- Camada recorrente básica (RNN)\n",
    "- Camada LSTM e GRU\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados sequenciais: recorrência\n",
    "\n",
    "### Para dados não sequenciais\n",
    "- Camadas densas e convolucionais consideram apenas o exemplo atual para computar a saída\n",
    "- Em cada iteração, cada entrada vai passando pelas camadas até atingir a saída\n",
    "\n",
    "<img src=\"Imagens/data1.png\" width=\"500\"> \n",
    "\n",
    "* Na iteração $t+1$ usamos os dados de $t+1$ para adaptar os parâmetros\n",
    "\n",
    "<img src=\"Imagens/data2.png\" width=\"500\"> \n",
    "\n",
    "### Para dados sequenciais\n",
    "- Se a iteração $t+1$ depende da anterior $t$, usamos a saída de cada camada para alimentar a camada na entrada da iteração $t+1$\n",
    "\n",
    "<img src=\"Imagens/data3.png\" width=\"500\"> \n",
    "\n",
    "* Dessa forma, a saída (após a primeira), dependerá não apenas da entrada atual, mas das saídas computadas anteriormente para cada unidade \n",
    "\n",
    "<img src=\"Imagens/data4.png\" width=\"500\"> \n",
    "\n",
    "Configurações de sequências\n",
    "- Uma entrada, saída sequencial\n",
    "    - Um áudio ou imagem é dado como entrada e a rede produz uma sequência de palavras que os descrevem\n",
    "- Entrada sequencial, uma saída\n",
    "    - Um texto é dado como entrada e a saída é sua análise de sentimentos: como positivo ou negativo\n",
    "- Entrada sequencial, saída sequencial\n",
    "    - Um texto é dado como entrada e a saída é uma tradução para outro idioma (com atraso k)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camada recorrente básica (RNN)\n",
    "\n",
    "Aprende um tipo de memória (memória de curto prazo) \n",
    "\n",
    "<img src=\"Imagens/rnn.png\" width=\"500\"> \n",
    "\n",
    "A equação é dada por: $h_t = f(h_{t-1}, x_t)$. Lembra uma MLP, mas pelo fato de conter o termo que remete ao termo da memória, é considerada uma rede recorrente\n",
    "\n",
    "Exemplo: Definindo uma codificação onte-hot para os caracteres\n",
    "- $h = [1, 0, 0, 0]$\n",
    "- $e = [0, 1, 0, 0]$\n",
    "- $l = [0, 0, 1, 0]$\n",
    "- $o = [0, 0, 0, 1]$\n",
    "\n",
    "<img src=\"Imagens/exrnn.png\" width=\"500\"> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM e GRU\n",
    "\n",
    "### Long Short Term Memory\n",
    "\n",
    "Foi criada para aumentar a capacidade de memória da unidade recorrente\n",
    "\n",
    "<img src=\"Imagens/lstm.png\" width=\"500\"> \n",
    "\n",
    "__Cell state__\n",
    "\n",
    "<img src=\"Imagens/cstate.png\" width=\"500\"> \n",
    "\n",
    "\n",
    "- É uma memória de longo prazo, adicionando contribuições para além da iteração anterior\n",
    "- Esse estado pode ser modificado por 2 gates\n",
    "\n",
    "__Forget gate__\n",
    "\n",
    "<img src=\"Imagens/forget.png\" width=\"500\"> \n",
    "\n",
    "- Decide o que cancelar de C com base no sumário anterior e a entrada atual\n",
    "- Saída entre 0 (esquecer) e 1(manter totalmente) para cada dimensão de C\n",
    "\n",
    "__Input gate__\n",
    "\n",
    "<img src=\"Imagens/input.png\" width=\"500\"> \n",
    "\n",
    "- Primeiro, combina o sumário anterior $h_{t-1}$ e a entrada atual $x_t$ para produzir um vetor $g_t$\n",
    "- Então, aprende um filtro que indica quais partes devem ser mantidas na \"memória longa\", sendo depois somado a $C_{t-1}$ para produzir $C_t$\n",
    "\n",
    "__Update cell state__\n",
    "\n",
    "<img src=\"Imagens/update.png\" width=\"500\"> \n",
    "\n",
    "\n",
    "- Agora temos uma combinação entre os estados atual e anterior \n",
    "- Acima o * significa uma multiplicação ponto a ponto\n",
    "\n",
    "__Output gate__\n",
    "\n",
    "<img src=\"Imagens/output.png\" width=\"500\"> \n",
    "\n",
    "- Decide qual será o sumário, transformado a partir do anterior\n",
    "- Ponderado de acordo com o estado de célula atual, $C_{t}$\n",
    "\n",
    "### Gate Recurrent Unit\n",
    "\n",
    "<img src=\"Imagens/gru.png\" width=\"500\"> \n",
    "\n",
    "- Não possui cell state\n",
    "- Reset gate r: decide o que cancelar de $h_{t-1}$ com base no sumário anterior e a entrada atual\n",
    "- Update gate z: pondera partes do sumário anterior de forma a complementar ao novo estado candidato \n",
    "- $\\hat{h_{t}}$ é o sumário \"candidato\"\n",
    "\n",
    "### GRU vs LSTM\n",
    "\n",
    "- Não há um consenso de qual o melhor método\n",
    "- GRU em muitos casos tem resultados similares à LSTM, com menos parâmetros\n",
    "- Há uma versão recente, JANET, que simplificou ainda mais o modelo, removendo o \"Reset gate\"\n",
    "- Temporal Convolutional Networks, que utilizam convoluções 1D para aprender posicionamento local, também se mostraram eficientes em alguns cenários\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
