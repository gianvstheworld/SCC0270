{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 5 TEORIA - Redes Convolucionais\n",
    "\n",
    "Aula 5 - Aula Assíncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Redes Convolucionais (CNNs)\n",
    "\n",
    "- 4 arquiteturas principais:\n",
    "    - Convolutional blocks\n",
    "    - Multi-branch blocks\n",
    "    - Residual blocks\n",
    "    - Separable Convolutions\n",
    "\n",
    "*obs: a maioria são CNNs desenvolvidas pro desafio ImageNet, ou seja, todas estão otimziadas pra resolver um problema de classificação de imagens com 1000 classes*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNS Sequênciais e em Blocos\n",
    "\n",
    "### AlexNet (Krizhevsky, 2012) - Sequencial\n",
    "\n",
    "*  entrada 224 × 224\n",
    "    * conv1: K = 96 filters with 11 × 11 × 3, stride 4,\n",
    "    * conv2: K = 256 filters with 5 × 5 × 96,\n",
    "    * conv3: K = 384 filters with 3 × 3 × 256,\n",
    "    * conv4: K = 384 filters with 3 × 3 × 384,\n",
    "    * conv5: K = 256 filters with 3 × 3 × 384,\n",
    "    * densas1, 2: K = 4096.\n",
    "    * <img src=\"Imagens/alexnet.jpg\" width=\"800\"> \n",
    "\n",
    "- Análises:\n",
    "    - Simples, porém pode ser melhorada\n",
    "    - O tamanho dos filtros vai diminuindo \n",
    "\n",
    "### VGGNet (Simonyan, 2014)\n",
    "\n",
    "- *obs: explicação mais detalhada> https://medium.com/analytics-vidhya/vggnet-architecture-explained-e5c7318aa5b6*\n",
    "\n",
    "* entrada 224 × 224,\n",
    "* filtros: todos 3 × 3,\n",
    "    * conv 1-2: K = 64 + maxpool\n",
    "    * conv 3-4: K = 128 + maxpool\n",
    "    * conv 5-6-7-8: K = 256 + maxpool\n",
    "    * conv 9-10-11-12: K = 512 + maxpool\n",
    "    * conv 13-14-15-16: K = 512 + maxpool\n",
    "    * densas1, 2: K = 4096\n",
    "    * <img src=\"Imagens/VGGnet.jpg\" width=\"800\"> \n",
    "\n",
    "- Análises:\n",
    "    - \"Evolução\" da AlexNet\n",
    "        - Ideia de manter uma rede sequencial com todos os filtros de mesmo tamanho\n",
    "    - 2 camadas 2 a 2 com o mesmo 3x3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura com ramos\n",
    "\n",
    "### GoogLeNet / Inception (Szegedy, 2014)\n",
    "\n",
    "* *https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7*\n",
    "\n",
    "- 22 layers (v1)\n",
    "    - Começa com duas camadas convolucionais\n",
    "    \n",
    "* Inception layer (banco de filtros):\n",
    "    * filtros 1 × 1, 3 × 3, 5 × 5 + max pooling 3 × 3;\n",
    "    * controla dimensionalidade usando filtros 1 × 1.\n",
    "    * 3 classificadores (não sequenciais)\n",
    "    * <img src=\"Imagens/GoogleLenet.png\" width=\"800\"> \n",
    "\n",
    "- esquema: \n",
    "    - Azul = conv.,\n",
    "    - Vermelho = pool.,\n",
    "    - Amarelo = densa+softmax,\n",
    "    - Verde = concatenação \n",
    "\n",
    "* Inception:\n",
    "    * bloco de divisão de dimensão e processamento em diferentes filtros com uma concatenação final\n",
    "        * reduz profundidade da entrada e concetena os mapas de ativação de diferentes filtros + maxpooling\n",
    "    * uma rede em uma rede\n",
    "    * <img src=\"Imagens/inception.jpg\" width=\"400\"> \n",
    "\n",
    "- Versões de Inception\n",
    "    - <img src=\"Imagens/modulesV2eV3.jpg\" width=\"400\"> \n",
    "    - a V3 é a mais utilizada (mais que V1, V2 e V4)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura residual\n",
    "\n",
    "* foi percebido (2014) que quanto maior a rede neural, melhor a performance da rede\n",
    "    * PORÉM, esse aumento da rede favorecia o Vanishing Gradient\n",
    "    * ou seja, para camadas mais próximas da entrada o gradiente tinha pouca importãncia na função custo final, causando poucas mudanças\n",
    "    * para redes muito grandes, isso causa um problema de adaptação\n",
    "    * <img src=\"Imagens/vanishing_gradient.jpg\" width=\"500\">\n",
    "\n",
    "\n",
    "### ResNet (He et al, 2015)\n",
    "\n",
    "* com isso, a proposta da ResNet de aproveitar como entrada na próxima camada da rede: saída da camada anteior + saída de 2 blocos anteriores\n",
    "    * assim, a rede era forçada a carregar com mais força as primeiras camadas, mitigando o vanishing gradient\n",
    "    * **skip layers** permite empilhar mais camadas (de 34 a 1000)\n",
    "    * <img src=\"Imagens/residual.jpg\" width=\"350\">\n",
    "    \n",
    "\n",
    "- arquitetura de uma ResNet\n",
    "    - <img src=\"Imagens/ResNet.jpg\" width=\"900\"> \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolução separáves em profundidade\n",
    "\n",
    "* Perder um pouco de acurácia (mais erro), porém, ganha muito em custo computacional\n",
    "\n",
    "### Depthwise separable convolutions - Xception, Mobilenet\n",
    "\n",
    "* Convolução convencional\n",
    "    * 3 × 3 × 3 × 128 = 1638 parâmetros.\n",
    "    * em imagem com 100 × 100 × 3 pixels, movemos cada filtro 10000 vezes para multiplicar os valores locais, total em multiplicações: 10000 × 1638 = 16.380.000\n",
    "    * <img src=\"Imagens/conv_convecional.jpg\" width=\"400\">\n",
    "\n",
    "- Convolução Separável: primeiro lateral, depois em profundidade\n",
    "    - Lateral executada uma única vez, produzindo um único volume, depois os filtros 1 × 1 × 3 produzirão p mapas de ativação\n",
    "    - <img src=\"Imagens/conv_separavel.jpg\" width=\"400\">\n",
    "\n",
    "* *obs: em vez de ir direto do bloco n x m x 3 para o n x m x 1, vamos para 3 blocos n x m x 1 que depois vão virar 1 bloco de n x m x 1*\n",
    "\n",
    "- Para obter 128 mapas de características com filtros 3 × 3 temos:\n",
    "    - lateral: 3 × 3 × (3) = 9 parâmetros.\n",
    "    - profundidade: 1 × 1 × 3 × (128) = 384 parâmetros\n",
    "    - na imagem 100 × 100 × 3, são 10000 posições,\n",
    "        - fase 1: 10000 × 9 = 90.000 multiplicações\n",
    "        - fase 2: 10000 × 384 = 3.840.000 multiplicações\n",
    "        - total = 3.930.000\n",
    "\n",
    "* A rede acaba não sendo tão boa em acurácia, porém, essa rede será muito mais rápida tanto em treinamento quanto em inferência\n",
    "    * Por isso, ela é conhecida como Mobilenet, pois foi a primeira a conseguir ser usada em um celular mobile, devido a eficiência\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
