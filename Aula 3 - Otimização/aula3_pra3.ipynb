{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 3 - Prática 3: Introdução à Otimização para Redes Neurais\n",
    "\n",
    "Aula 3 - Aula Assíncrona\n",
    "https://www.youtube.com/watch?v=O3h4kUdSV4k&t=2s&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "---\n",
    "\n",
    "## Otimização para Redes Neurais\n",
    "\n",
    "- Redes Densas em pytorch e detalhes:\n",
    "    - Camadas pré-implementadas\n",
    "    - Opções para acessar parâmetros\n",
    "    - Uso do gradiente\n",
    "    - Backpropagation e algoritmos de otimização\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto Rede Densa\n",
    "\n",
    "- Exemplo da aula teórica\n",
    "    - Entrada: imagens 28x28 pixels\n",
    "    - Saída: 10 classes (dígitos de 0 a 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module): # herda uma classe com diversos parâmetros encapsulados\n",
    "\n",
    "    def __init__(self):\n",
    "        # inicialização conforme a superclasse nn.Module\n",
    "        super(Network, self).__init__()\n",
    "        # cria o projeto da rede neural\n",
    "        # duas camadas Fully Connected (camadas densas), fc1 = hidden e fc2 = final\n",
    "        self.fc1 = nn.Linear(784, 32) # entrada = 784, saída = 32\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # achatar todas as camadas, exceto uma\n",
    "        x = torch.flatten(x, 1) # recebe minibatches\n",
    "        x = F.relu(self.fc1(x)) # camada densa (processa os dados de x) + relu\n",
    "        x = self.fc2(x) # camada densa linear (sem softmax por enquanto)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2961, -0.1091,  0.5809, -0.0283, -0.1638,  0.0520,  0.1414, -0.1080,\n",
       "         -0.0909, -0.0965]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_random = torch.randn(1, 1, 28, 28) # 1 batch com 1 canal de cor 28x28 = imagem aleatoria\n",
    "print(input_random.shape)\n",
    "\n",
    "output = net(input_random)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Acessando os parâmetros da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([32, 784])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0243, -0.0092,  0.0023,  ..., -0.0019,  0.0040, -0.0288],\n",
      "        [ 0.0141, -0.0345,  0.0016,  ..., -0.0143,  0.0107, -0.0199],\n",
      "        [-0.0244, -0.0241, -0.0177,  ..., -0.0020,  0.0112, -0.0288],\n",
      "        ...,\n",
      "        [ 0.0291, -0.0108,  0.0162,  ...,  0.0101,  0.0351, -0.0249],\n",
      "        [-0.0114,  0.0357, -0.0221,  ..., -0.0325, -0.0244,  0.0228],\n",
      "        [-0.0318,  0.0357,  0.0213,  ..., -0.0258, -0.0139,  0.0129]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "# primeira camada\n",
    "print(params[0].size())\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "* Já está pronto: `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zerar o buffer de gradientes\n",
    "net.zero_grad()\n",
    "output.backward(torch.randn(1,10)) # aleatorio para exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede operando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2478, -0.3516, -0.9182, -1.0940,  0.5680, -0.2292, -0.9411, -0.1779,\n",
      "         0.1158,  1.4357])\n",
      "tensor([[ 1.2478, -0.3516, -0.9182, -1.0940,  0.5680, -0.2292, -0.9411, -0.1779,\n",
      "          0.1158,  1.4357]])\n",
      "tensor(1.0007, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input_random)\n",
    "target = torch.randn(10) # target aleatorio como exemplo\n",
    "print(target)\n",
    "target = target.view(1, -1) # um elemento em uma dimensão e o resto em outra\n",
    "print(target)\n",
    "\n",
    "criterion = nn.MSELoss() # mean squared error\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7f569b14ee50>\n",
      "<AddmmBackward0 object at 0x7f569b14ef40>\n",
      "<AccumulateGrad object at 0x7f569b14ee50>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0]) # func linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # func relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation com `backwards`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Depois: tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0628,  0.0000,  0.0760,  0.0000,\n",
      "         0.0000,  0.0083, -0.0303,  0.0396,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0981,  0.0000, -0.0217,  0.0000, -0.0223,  0.0306,  0.0000,\n",
      "         0.0862,  0.0355,  0.0780,  0.0370,  0.0715,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # tem que zerar o gradiente \n",
    "# antes\n",
    "print('Antes:', net.fc1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('Depois:', net.fc1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizadores\n",
    "\n",
    "- Adaptacao dos parâmetros da rede\n",
    "    - Otimziador: SGD, Adam, entre outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0243, -0.0092,  0.0023, -0.0185,  0.0247,  0.0155,  0.0332,  0.0174,\n",
      "        -0.0116, -0.0058], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# cria objeto otimizador\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# para cada loop de treinamento\n",
    "\n",
    "# zerar o buffer dos grads\n",
    "optimizer.zero_grad()\n",
    "# gerar a saida e computar gradientes com relacao a func de perda\n",
    "output = net(input_random)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "# realiza a adaptacao dos pesos\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0243, -0.0092,  0.0023, -0.0185,  0.0247,  0.0155,  0.0332,  0.0174,\n",
      "        -0.0116, -0.0058], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
