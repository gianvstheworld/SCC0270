{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 3 TEORIA - Otimização para Redes Neurais\n",
    "\n",
    "Aula 3 - Aula Presencial\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptrons e Multi-layer Perceptrons\n",
    "\n",
    "- Funções de custo\n",
    "- Convexidade\n",
    "- Gradiente\n",
    "- Descida do Gradiente Estocástica\n",
    "- Mini-batches e taxa de aprendizado\n",
    "- Momentum e Adam\n",
    "- Decaimento da taxa de aprendizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Básico de Treinamento de Redes Neurais\n",
    "\n",
    "* Idealmente deve ser convexa (facilita na existência de um mínimo global)\n",
    "\n",
    "- Funções destaques:\n",
    "    - **Mean-squared-error:** \n",
    "        - Para avalores contínuos\n",
    "        - Mede a divergência quadrática de cada valor obtido com a saída real\n",
    "    - **Cross-Entropy:**\n",
    "        - Recomendada para probabilidades\n",
    "        - Teoria da informação \n",
    "        - Intuição -> o número de bits adicionais necessários para representar o evento de referência ao invés do predito\n",
    "\n",
    "* Gradient descent -> buscar mínimo global (ou o mínimo local melhor na região em que se encontra)\n",
    "    *  <img src=\"Imagens/gradient_descent.jpg\" width=\"400\"> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward e Backward Propagation\n",
    "\n",
    "### Foward-propagation\n",
    "* O que é?\n",
    "    * Cálculo e armazenamento das variáveis intermediárias (incluindo saídas)\n",
    "    * Entrada -> saída\n",
    "\n",
    "- Como funciona?\n",
    "    - Considerando uma entrada x, bias b = 0, cada camada com pesos de conexão w\n",
    "\n",
    "- Camada oculta com k neuronios\n",
    "    - Variável intermediária z = W1 * x \n",
    "        - obs: W é uma matriz\n",
    "    - Vetor de ativação da camada oculta: h = f(z)\n",
    "    - O output da camada de saída é: o = W2 * h\n",
    "    - Custo -> L = l(o, y) = (y - o)^2\n",
    "\n",
    "* Grafo:\n",
    "    * <img src=\"Imagens/fowardpropagation.jpg\" width=\"400\"> \n",
    "\n",
    "### Back-propagation\n",
    "- O que é?\n",
    "    - Propagação que utiliza a derivada ao longo das camadas para adaptar os pesos\n",
    "    - As funções de custo e de ativação devem produzir derivada útil \n",
    "\n",
    "* Detalhamento\n",
    "    * Calcular o gradiente dos parâmetros da rede neural\n",
    "    * Atravessa a rede em ordem reversa, da saída para a entrada\n",
    "    * Saída -> entrada\n",
    "    * Utiliza a regra da cadeia\n",
    "    * Armazena as derivadas parciais das variáveis intermediárias com relação aos parâmetros\n",
    "\n",
    "- Como funciona?\n",
    "    - Considerando uma entrada x, bias b = 0, cada camada com pesos de conexão w:\n",
    "\n",
    "- Camada oculta com k neuronios\n",
    "    - Sejam os parâmetros da rede W1 e W2; o backpropagation calcula os gradiente do custo em relação a eles\n",
    "        - del L /del W1\n",
    "    - Regra da cadeia de del L / del W1 = \n",
    "        - del L / del o\n",
    "        - del o / del f(z)\n",
    "        - del f(z) / del z\n",
    "        - del z / W1\n",
    "            - Multiplica tudo: del L / del W1 = del L / del o * del o / del f(z) * del f(z) / del z * del z / W1\n",
    "\n",
    "* Grafo:\n",
    "    * <img src=\"Imagens/backpropagation.jpg\" width=\"400\"> \n",
    "\n",
    "- **Vanishing gradient**\n",
    "    -  Se ativações geram valores muito baixos não é possível adaptar\n",
    "    -  Esse é um dos motivadores do uso de ReLU ao invés de Sigmóides como função de ativação\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklists para o treinamento\n",
    "\n",
    "### Inicialização\n",
    "\n",
    "- Escolhas comuns:\n",
    "    - Pesos w: valor aleatório pela distribuiçaõ normal entre 0 e 1 (média 0 e desvio padrão 1)\n",
    "    - Bias: 0 (as vezes com 1/total de classes)\n",
    "\n",
    "### Checklist 1\n",
    "\n",
    "* O valor da função de custo nos pesos aleatórios faz sentido?\n",
    "\n",
    "### Checklist 2\n",
    "\n",
    "* Decaimento de taxa de aprendizado\n",
    "    * De forma fixa ou de acordo com métricas computadas no treinamento ou validação?\n",
    "\n",
    "### Checklist 3\n",
    "\n",
    "* Utilizar qual otimziador?\n",
    "    * SGD (+ Momentum)\n",
    "    * Adam\n",
    "        * Esses são mais comuns e robustos\n",
    "\n",
    "### Checklist 4\n",
    "\n",
    "* Acompanhe o custo ao longo de épocas, se possível com conjunto de validação (idealmente não deve ser o teste!)\n",
    "\n",
    "-  Inicie com experimentos com poucos exemplos\n",
    "    - Explore os hiperparâmetros tentando obter \"overfitting\" para um subconjunto de exemplos, obtendo custo próximo a zero, e depois refine a busca num conjunto maior\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizadores, tamanho do batch e taxa de aprendizado\n",
    "\n",
    "### Otimizadores\n",
    "\n",
    "* Técnicas para otimizar a rede neural\n",
    "    * Nesse caso mais relacionadas com o batch\n",
    "    * Exemplo: https://gbhat.com/machine_learning/optimize_with_momentum.html \n",
    "\n",
    "* **Batch Gradient Descent**\n",
    "    * Todo o batch é utilizado para o treinamento em um único passo (a média do gradient é utilizado)\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**\n",
    "    - Para cada época, uma amostra do batch é escolhida aleatoriamente (estocático) para calcular o grandient descent\n",
    "    - Atualização por instãncia\n",
    "    - $O$ j = $O$ j - $a$ ($y^{i}$ - $y_{i}$)($x_{ij}$)\n",
    "        - $a$ é o learning rate\n",
    "    - <img src=\"Imagens/SBG_merged.png\" width=\"800\">\n",
    "    - o SGD é mais \"ruidoso\" por natureza e tem dificuldade em achar minimos globais, mas apresenta uma solução pouco custosa computacionalmente\n",
    "\n",
    "\n",
    "* **Mini Batch Gradient Descent (MBGD)**\n",
    "    * Meio termo: uma parte aleatório das amostras do dataset é usado para calcular o gradient descent\n",
    "    * $O_{j}$ = $O_{j}$ - $a$ g($X_{j}$, $O_{j}$)\n",
    "\n",
    "- Mudamos um pouco de escopo daqui para baixo, vamos falar de outros otimizadores:\n",
    "\n",
    "* **Momentum**\n",
    "    * Custo = terreno montanhoso\n",
    "        * Inicio: particula com v = 0\n",
    "        * Otimização: rolar a particula considerando a aceleração\n",
    "        * Consequencia: velocidade ajustada considerando as atualizações anteriores\n",
    "        * <img src=\"Imagens/momentum.jpg\" width=\"300\">\n",
    "\n",
    "- **Adam**\n",
    "    - Utiliza momentos do gradiente: o segundo momento é usado para normalizar o primeiro, evitando outliers/pontos de inflexão\n",
    "    - Funciona melhor com taxa de aprendizado menor, quando comparado ao SGD\n",
    "    -  <img src=\"Imagens/adam.jpg\" width=\"250\">\n",
    "\n",
    "### Tamanho do Batch\n",
    "\n",
    "* Padrão de batch é 32 \n",
    "    * **Batches maiores:** estimativas mais suaves, difícil manter na memória, exige ajustar bem a **taxa de aprendizado**\n",
    "    * **Batches menores**: estimativas mais ruidosas, mas que mostraram vantagens em encontrar melhores mínimos\n",
    "\n",
    "### Taxa de aprendizado\n",
    "\n",
    "* Padrão é 0.01\n",
    "    * Pouco adequado para alguns otimizadores\n",
    "    * Pode ser pouco adequado para batchs muito grandes ou muito pequenos\n",
    "\n",
    "-  É recomendado iniciar com um valor maior e reduzir a taxa progressivamente (learning rate scheduling)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
