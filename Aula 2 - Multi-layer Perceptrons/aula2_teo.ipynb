{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 2 TEORIA - Multi-Layer Perceptron\n",
    "\n",
    "Aula 2 - Aula Assíncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptrons e Multi-layer Perceptrons\n",
    "\n",
    "- Modelos lineares, implementação básica em pytorch e inicialização\n",
    "- Perceptron\n",
    "- Multi-layer perceptrons\n",
    "- Forward propagation, backward propagation\n",
    "- Entropia Cruzada\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão e Classificação\n",
    "\n",
    "### Modelo linear\n",
    "\n",
    "*obs: Feature = atrbibuto = entrada*\n",
    "\n",
    "* Para uma feature:\n",
    "    * y = w x + b\n",
    "\n",
    "- Para mais features (2 ou mais):\n",
    "    - y = w1 x1 + w2 x2 + b\n",
    "\n",
    "* para n feautres:\n",
    "    * y = w1 x1 + ... wn xn + d\n",
    "    * Pode ser usada notação matricial para facilitar\n",
    "\n",
    "- **Notação matricial:**\n",
    "    - y = w<sup>T</sup> x + b\n",
    "\n",
    "* Para cada elemento: y = X w + b\n",
    "\n",
    "*obs: o último b vai ser somado com broadcasting*\n",
    "\n",
    "* Temos uma l<sup>(i)</sup>(w,b) para cada medida e uma L(w,b) para tudo\n",
    "    * Usaremos normalmente a perda quadrática\n",
    "\n",
    "- Buscamos os parametros que minimizam a perda média com o treinamento: \n",
    "    - w', b' = arg min L(w,b)\n",
    "\n",
    "* Usaremos o MSGD (minibatch stochastic gradient descent):\n",
    "    * Minibatch de tamanho B\n",
    "    * Passo (agressividade) $a$\n",
    "    * Iniciar parâmetros (aleatoriamente)\n",
    "    * Usar o negativo do gradiente para **atualizar** os parâmetros\n",
    "\n",
    "![alt text](graddesc.jpg \"Title\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "* Até agora estavámos falando de algoritmos lineares gerais, para agora vamos falar de algoritmos lineares de redes neurais(baseados em neurônios)\n",
    "    * Começaremos pela mais simples e clássica Perceptron\n",
    "\n",
    "- Neurônio\n",
    "    - Entrada: vetor \n",
    "    - Saída: valor único\n",
    "        - Peso w (conexão) e bias b (intercepto)\n",
    "    - Treinar é ajustar cada w e b\n",
    "\n",
    "*obs: há uma função de ativação que é aplicada nessa soma*\n",
    "\n",
    "<img src=\"neuronio.jpg\" width=\"300\"/>\n",
    "\n",
    "* Função de ativação\n",
    "    * Exemplo: Sigmoid, ReLU e Leaky ReLU\n",
    "    * No geral elas limitam o score entre 0 e 1 (facilita a comparação) e trunca entre alto e baixo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "* Expandir a perceptron\n",
    "    * Exemplo: classificação de dígitos (0 a 9)\n",
    "        * Entrada com imagens 28x28 = 784 pixels (features)\n",
    "        * Perceptron\n",
    "        * SGD com 32 imagens no batch\n",
    "        * Saída: 10 classes (0 a 9)\n",
    "\n",
    "### Classificador\n",
    "\n",
    "- Nova função de ativação: Softmax\n",
    "    - Interessante para normalizar a saída de forma a somar 1\n",
    "    - Interpretar cada valor como uma probabilidade\n",
    "    - Distribuição de probabilidades das classes i\n",
    "    - <img src=\"softmax.jpg\" width=\"250\">\n",
    "\n",
    "* O rótulo com o valor real passa a ter 1 e 0 (one hot encoding)\n",
    "    * y = [0.0 0.25 0.75] (medida)\n",
    "    * y' = [0 1 0] (rótulo)\n",
    "\n",
    "### Rede Neural Rasa\n",
    "\n",
    "* Uma única camada: \n",
    "\n",
    "<img src=\"NN_4_class.jpg\" width=\"350\">\n",
    "\n",
    "- Exemplo do digito\n",
    "    - Entrada: 784 (pixels)\n",
    "    - Saída: 10 (0 a 9)\n",
    "    - Batch: 32\n",
    "\n",
    "* y = X W + b\n",
    "    * X tem tamanho 32x784 (linha = imagem, coluna = pixel)\n",
    "    * y tem tamanho 10\n",
    "    * Logo -> [ 10 ] = [32 x 784] * W + b\n",
    "        * Portanto, **W tem tamanho [784 x 10] e b tem tamanho [ 10 ]**\n",
    "    \n",
    "- <img src=\"exemplo_digito.jpg\" width=\"400\">\n",
    "- y será uma matriz de 32 linhas por 10 colunas, sendo cada coluna uma probabilidade\n",
    "- Somando as 10 colunas em cada linha, tmeos 1 (probabilidade 100%)\n",
    "\n",
    "*obs: lembrando que são 32 linhas, porque queremos classificar 32 imagens, ou seja, queremos uma resposta da rede para cada imagem*\n",
    "\n",
    "### Rede Neural Profunda\n",
    "\n",
    "- Adição de camadas ocultas para dividr o problema em partes menores\n",
    "- <img src=\"camadas_NN.jpg\" width=\"400\">\n",
    "\n",
    "* **Feed forward:** todos os neurônios processam a entrada, e computamos a perda\n",
    "\n",
    "- **Backpropagation:** algoritmo que usa a regra da cadeia para calcular o gradiente da função de perda, e propaga esse gradiente pelas camadas e neurônios\n",
    "    - Sempre da última para primeira (assim já sabemos o resultado da classificação e o erro)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação do Perceptron\n",
    "\n",
    "* Cada neuronio da saida é uma reta em um plano de N dimensões e cada objeto a ser classificado é um ponto nesse espaço\n",
    "* <img src=\"grafico.jpg\" width=\"300\">\n",
    "* Exemplo para caso binário: duas possibilidades de classes\n",
    "\n",
    "### Perda Entropia Cruzada\n",
    "\n",
    "* \"Para codificar dados amostrados de P precisamos ao menos de H[P] bits\", sendo:\n",
    "* H[P] a entropia de uma distribuição P:\n",
    "    * <img src=\"entropia.jpg\" width=\"300\">\n",
    "\n",
    "### Log-Likelihood (verossimilhança)\n",
    "\n",
    "* Softmax gera um vetor y^ com probabilidades condicionais para cada classe\n",
    "    * y^ = P(y = 'gato' | x), ou seja, y é a probabilidade da imagem y ser um gato dado uma imagem x\n",
    "\n",
    "- Podemos comparar yˆ com y verificando quão provável as classes reais são com relação à nosso modelo dadas as entradas:\n",
    "    - P(Y|X) = Produtório de P(yi | xi), de i até n\n",
    "    - Precisamos maximizar P(Y|X), para isso vamos utilizar log pra converter para um problema de minimização (aplicar -log dos dois lados)\n",
    "\n",
    "* Assim, para cada par de rótulo y e a predição do modelo y^ ao longo de c classes, o custo é:\n",
    "    * <img src=\"likelihood.jpg\" width=\"250\">\n",
    "\n",
    "- Exemplo:\n",
    "    - y = [0, 1, 0] e y^ = [0, 0.25, 0.75]\n",
    "    - l(y, y^) = \n",
    "        - 0.log(0) = 0\n",
    "        - 1.log(0.25) = -1*(-0.6) = 0.6\n",
    "        - 0.log(0.75) = 0\n",
    "    - l(y,y^) = 0.6\n",
    "        - Portanto, temos uma likelihood de 0.6, o que não é muito bom no fim das contas\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
