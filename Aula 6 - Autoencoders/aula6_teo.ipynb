{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 6 TEORIA - Autoencoders\n",
    "\n",
    "Aula 6 - Aula Assíncrona\n",
    "https://www.youtube.com/watch?v=O6C6wl357-o\n",
    "\n",
    "---\n",
    "\n",
    "## Redes neurais auto-associativas (Autoencoders)\n",
    "\n",
    "- Autoencoders\n",
    "- \"Undercomplete\"\n",
    "- \"Overcomplete\"\n",
    "- Denoising autoencoders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "Redeus neurais auto-associativas, ou auto-encoders, são métodos não supervisionados para aprendizado de representações.\n",
    "\n",
    "<img src=\"Imagens/ae.png\" width=\"500\"> \n",
    "\n",
    "- A entrada é passada por algum sistema (nesse caso a rede neural) e esse sistema de alguma maneira tentará reproduzir a saída \n",
    "    - Encoder: pegar a entrada x e transformar em um código (esse código pode ser interpretado como uma versão de x comprimida)\n",
    "        - Representação latente ou feature embedding\n",
    "        - $h = s(Wx + b) = f(x)$\n",
    "    - Decoder: a partir desse código reconstruir a entrada\n",
    "        - $\\hat{x} = s(W'h + b') = g(h)$\n",
    "- A reconstrução feita nunca será perfeita\n",
    "\n",
    "### Função de perda\n",
    "\n",
    "A partir da saída $\\hat{x} = g(f(x))$ minimizamos o erro/perda relativa à reconstrução da entrada\n",
    "\n",
    "### Tipos\n",
    "Seja um código h pertencente a $R^{m}$\n",
    "\n",
    "- Undercomplete\n",
    "    - $m$ poussi menos dimensões que $x$\n",
    "    - A camada do código é chamada de gargalo ou \"bootleneck\" por ser restrita\n",
    "- Overcomplete\n",
    "    - $m$ possui dimensões maiores ou iguais às de $x$\n",
    "    - Há diferentes versões desse tipo para compensar a falta de restrição no código\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete\n",
    "\n",
    "- Código é uma compressão com perdas da entrada\n",
    "    - camda do código é chamada de \"bottleneck\"\n",
    "    - o código produz boa representação para os dados de treinamento, em particular reconstrução\n",
    "\n",
    "<img src=\"Imagens/undercomplete.png\" width=\"500\"> \n",
    "\n",
    "- Pode ser usado para aprender uma __redução de dimensionalidade__\n",
    "- Um autoencoder denso com uma única camada encoder/decoder tem relações com o método Principal Component Analysis (PCA)\n",
    "- Se a variedade (manifold) dos dados é linear, o AE tende a convergir para uma projeção nos $m$ principais componente\n",
    "\n",
    "* Autoencoders profundos \n",
    "    * Camadas não densas, ex: convolucionais, pooling, etc\n",
    "    * Camada do código é comumente densa para permitir projeção dos dados\n",
    "* Modelo U-net\n",
    "    * Entrada passa por uma sequência de camadas convolucionais até chegar em um código no qual a ela vai aumentando sua dimensionalidade até chegar na entrada original\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcomplete\n",
    "\n",
    "* Camada intermediária com alta dimensionalidade \n",
    "    * uma implementação simples permitiria a cópia simples (e perfeita)dos dados de forma que $x = \\hat{x}$ \n",
    "\n",
    "<img src=\"Imagens/overcomplete.png\" width=\"350\"> \n",
    "\n",
    "Uma maneira de impedir a cópia é a regularização com alguma função $R(.)$, ex: regularização L1\n",
    "- A função de custo tenta manter um baixo número de ativações por entrada\n",
    "- Dropout também pode ser usado, nesse caso imediatamente antes da camada do código\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising autoencoders\n",
    "\n",
    "A regularização é atingida adicionando ruído à entrada, $\\hat{x} = N(x)$\n",
    "- a perda é computada usando a entrada não ruidosa x\n",
    "- AE aprende a reconstruir x a partir de $\\hat{x}$\n",
    "- O encoder deve aprender a remover o ruído, mantendo apenas as informações essenciais no código, permitindo que o decoder reconstrua a entrada\n",
    "\n",
    "* DAEs aprendem uma representação robusta como efeito colateral de aprender a remover o ruído da entrada\n",
    "\n",
    "<img src=\"Imagens/denoising.png\" width=\"350\"> \n",
    "\n",
    "### Como adicionar ruído? Prática comum\n",
    "- Comumente ruído Gaussiano/Normal\n",
    "- Ruído impulsivo: atribuir zero a uma porcentagem da entrada, com probabilidade p (dropout na entrada)\n",
    "\n",
    "### Intuição\n",
    "* Aprende a projetar os dados ao longe de uma variedade/manifold relativo aos dados de entradas originais\n",
    "* Apenas entradas fora da distribuição original gerarão alto erro de reconstrução\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
